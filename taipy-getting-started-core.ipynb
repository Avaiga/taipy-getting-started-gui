{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5632b274",
   "metadata": {},
   "source": [
    "# Taipy Core\n",
    "\n",
    "Taipy Core is one of the components of Taipy to facilitate pipeline orchestration. There are a lot of reasons for using Taipy Core:\n",
    "\n",
    "- Taipy Core efficiently manages the execution of your functions/pipelines.\n",
    "\n",
    "- Taipy Core manages data sources and monitors KPIs.\n",
    "\n",
    "- Taipy Core provides easy management of multiple pipelines and end-user scenarios, which comes in handy in the context of Machine Learning or Mathematical optimization.\n",
    "\n",
    "To apprehend the Scenario Management aspect of Taipy, you need to understand four essential concepts.\n",
    "\n",
    "\n",
    "## Four fundamental concepts in Taipy Core:\n",
    "- Data Nodes: are the translation of variables in Taipy. Data Nodes don't contain the data but know how to retrieve it. They can refer to any data: any Python object (string, int, list, dict, model, dataframe, etc), a Pickle file, a CSV file, an SQL database, etc. They know how to read and write data. You can even write your own custom Data Node if needed to access a particular data format.\n",
    "\n",
    "- Tasks: are the translation of functions in Taipy.\n",
    "\n",
    "- Pipelines: are a list of tasks executed with intelligent scheduling created automatically by Taipy. They usually represent a sequence of Tasks/functions corresponding to different algorithms like a simple baseline Algorithm or a more sophisticated Machine-Learning pipeline.\n",
    "\n",
    "- Scenarios: End-Users very often require modifying various parameters to reflect different business situations. Taipy Scenarios will provide the framework to \"play\"/\"execute\" pipelines under different conditions/variations (i.e., data/parameters modified by the end user)\n",
    "\n",
    "\n",
    "## What is a configuration?\n",
    "\n",
    "Configuration is the structure or model of what is our scenario. It represents our Direct Acyclic Graph but also how we want our data to be stored or how our code is run. Taipy is able to create multiple instances of this structure with different data thus, we need a way to define it through this configuration step.\n",
    "\n",
    "\n",
    "## Taipy Studio for configuration\n",
    "\n",
    "There are two ways to configure Taipy Core, either by Python code or with Taipy Studio. We strongly recommend using Taipy Studio. \n",
    "\n",
    "Taipy Studio is a VS Code extension that provides a graphical editor to describe pipelines. Everything can be done easily and faster through Taipy Studio. \n",
    "\n",
    "\n",
    "Let's create our first configuration and then create our entities to submit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676c8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!rmdir /s /q .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0587baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taipy import Config\n",
    "import taipy as tp\n",
    "\n",
    "# Normal function used by Taipy\n",
    "def double(nb):\n",
    "    return nb * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7946a7",
   "metadata": {},
   "source": [
    "Here is the code to configure a simple scenario.\n",
    "\n",
    "Two Data Nodes are being configured 'input' and 'output'. The 'input' Data Node has a _default_data_ put at 21. They will be stored as Pickle files automatically and they are unique to their scenario.\n",
    "\n",
    "The task links the two scenarios through the Python function _double_.\n",
    "\n",
    "The pipeline will contain this one task and the scenario will contain this one pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3695",
   "metadata": {},
   "source": [
    "______________________ Taipy Studio ______________________\n",
    "- Create new file: 'config.toml'\n",
    "- Open Taipy Studio view\n",
    "- Go to the 'Config files' section of Taipy Studio\n",
    "- Right click on the right configuration\n",
    "- Choose 'Taipy: Show View'\n",
    "- Add your first Data Node by clicking the button on the right above corner of the windows\n",
    "- Create a name for it and change its details in the 'Details' section of Taipy Studio\n",
    "        - name: input\n",
    "        - Details: default_data=21, storage_type:pickle\n",
    "- Do the same for the output\n",
    "        - name: output\n",
    "        - Details: storage_type:pickle\n",
    "- Add a task and choose a function to associate with `<module>.<name>:function`\n",
    "        -name: double\n",
    "        -Details: function=`__main__.double:function`\n",
    "- Link the Data Nodes and the task\n",
    "- Add a pipeline and link it to the task\n",
    "- Add a scenario and link to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814893c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of Data Nodes\n",
    "input_data_node_cfg = Config.configure_data_node(\"input\", default_data=21)\n",
    "output_data_node_cfg = Config.configure_data_node(\"output\")\n",
    "\n",
    "# Configuration of tasks\n",
    "task_cfg = Config.configure_task(\"double\",\n",
    "                                 double,\n",
    "                                 input_data_node_cfg,\n",
    "                                 output_data_node_cfg)\n",
    "\n",
    "# Configuration of the pipeline and scenario\n",
    "pipeline_cfg = Config.configure_pipeline(\"my_pipeline\", [task_cfg])\n",
    "scenario_cfg = Config.configure_scenario(\"my_scenario\", [pipeline_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3494889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-22 16:20:02,740][Taipy][INFO] job JOB_double_699613f8-7ff4-471b-b36c-d59fb6688905 is completed.\n",
      "Value at the end of task 42\n"
     ]
    }
   ],
   "source": [
    "# Run of the Core\n",
    "tp.Core().run()\n",
    "\n",
    "# Creation of the scenario and execution\n",
    "scenario = tp.create_scenario(scenario_cfg)\n",
    "tp.submit(scenario)\n",
    "\n",
    "print(\"Value at the end of task\", scenario.output.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3f345",
   "metadata": {},
   "source": [
    "## Basic functions\n",
    "\n",
    "Let's discuss about the basic functions that comes along with Taipy.\n",
    "\n",
    "-_write_: this is how data can be changed through Taipy. _write_ will change the _last_edit_date_ of the data node which will influence if a task can be skipped or not.\n",
    "\n",
    "-_tp.get_scenarios()_: this function returns a list of all the scenarios\n",
    "\n",
    "-_tp.get()_: this function returns an entity based on the id of the entity\n",
    "\n",
    "-_tp.delete(ID)_: this function deletes the entity and nested elements based on the id of the entity\n",
    "\n",
    "## Utility of having scenarios\n",
    "\n",
    "Taipy lets the user create multiple instances of the same configuration. Datas can differ between instances and can be used to compare different scenarios.\n",
    "\n",
    "Datas can be naturally different depending on the input data nodes or the randomness of functions. Moreover, the user can change them with the _write_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23912c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-22 16:20:02,874][Taipy][INFO] job JOB_double_a5ecfa4d-1963-4776-8f68-0859d22970b9 is completed.\n",
      "First submit 42\n"
     ]
    }
   ],
   "source": [
    "scenario = tp.create_scenario(scenario_cfg, name=\"Scenario\")\n",
    "tp.submit(scenario)\n",
    "print(\"First submit\", scenario.output.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20db8b",
   "metadata": {},
   "source": [
    "By using _write_, data of a Data Node can be changed. The syntax is `<Scenario>.<Pipeline>.<Data Node>.write(value)`. If there is just one pipeline, we can just write `<Scenario>.<Data Node>.write(value)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9d25c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before write 21\n",
      "After write 54\n"
     ]
    }
   ],
   "source": [
    "print(\"Before write\", scenario.input.read())\n",
    "scenario.input.write(54)\n",
    "print(\"After write\",scenario.input.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067da66",
   "metadata": {},
   "source": [
    "The submission of the scenario will update the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1f60e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-22 16:20:03,011][Taipy][INFO] job JOB_double_7eee213f-062c-4d67-b0f8-4b54c04e45e7 is completed.\n",
      "Second submit 108\n"
     ]
    }
   ],
   "source": [
    "tp.submit(scenario)\n",
    "print(\"Second submit\",scenario.output.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6221d491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 54]\n"
     ]
    }
   ],
   "source": [
    "# Basic functions of Taipy Core \n",
    "# how to access all the scenarios\n",
    "print([s.input.read() for s in tp.get_scenarios()])\n",
    "\n",
    "# how to get a scenario from its id\n",
    "scenario = tp.get(scenario.id)\n",
    "\n",
    "# how to delete a scenario\n",
    "tp.delete(scenario.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb408343",
   "metadata": {},
   "source": [
    "## Different types of Data Nodes:\n",
    "\n",
    "- *Pickle* (default): Taipy can store and read anykind of data that can be serializable.\n",
    "\n",
    "- *CSV*: Taipy can read and store any dataframe as a CSV.\n",
    "\n",
    "- *JSON*: Taipy can read and store any JSONable data as a JSON file.\n",
    "\n",
    "- *SQL*: Taipy can read and store a table or data base.\n",
    "\n",
    "- *Generic*: Taipy provides a generic Data Node that can read and store any data based on the reding and writing function created by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebf9e1",
   "metadata": {},
   "source": [
    "The execution graph used to explain the different concepts is quite simple.\n",
    "\n",
    "Three Data Nodes:\n",
    "- historical data: initial CSV DataFrame\n",
    "- month_data: DataFrame after the filtering on the month (pd.DataFrame as a Pickle file)\n",
    "- nb_of_values: number of values in this month (int as a Pickle file)\n",
    "\n",
    "Two tasks linking these Data Nodes:\n",
    "- filter: filters on the months of the dataframe\n",
    "- count_values: calculates the number of elements in this month\n",
    "\n",
    "One pipeline in a scenario gathering these two tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5668d",
   "metadata": {},
   "source": [
    "![](config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df321fae",
   "metadata": {},
   "source": [
    "______________________ Taipy Studio ______________________\n",
    "- Create new file: 'config.toml'\n",
    "- Open Taipy Studio view\n",
    "- Go to the 'Config files' section of Taipy Studio\n",
    "- Right click on the right configuration\n",
    "- Choose 'Taipy: Show View'\n",
    "- Add your first Data Node by clicking the button on the right above corner of the windows\n",
    "- Create a name for it and change its details in the 'Details' section of Taipy Studio\n",
    "        - name: historical_data\n",
    "        - Details: default_path=xxxx/yyyy.csv, storage_type=csv\n",
    "- Do the same for the month_data and nb_of_values\n",
    "        - name: month_data and nb_of_values\n",
    "        - Details: storage_type:pickle\n",
    "- Add a task and choose a function to associate with `<module>.<name>:function`\n",
    "        -name: filter_current\n",
    "        -Details: function=`__main__.filter_current:function`\n",
    "- Do the same for count_values\n",
    "- Link the Data Nodes and the tasks\n",
    "- Add a pipeline and link it to the tasks\n",
    "- Add a scenario and link to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5edc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b4a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_current(df):\n",
    "    current_month = dt.datetime.now().month\n",
    "    df['Date'] = pd.to_datetime(df['Date']) \n",
    "    df = df[df['Date'].dt.month == current_month]\n",
    "    return df\n",
    "\n",
    "def count_values(df):\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57040600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a CSV Data Node\n",
    "historical_data_cfg = Config.configure_csv_data_node(id=\"historical_data\",\n",
    "                                                     default_path=\"time_series.csv\")\n",
    "month_values_cfg =  Config.configure_data_node(id=\"month_data\")\n",
    "nb_of_values_cfg = Config.configure_data_node(id=\"nb_of_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b3f36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_filter_current_cfg = Config.configure_task(id=\"filter_current\",\n",
    "                                                 function=filter_current,\n",
    "                                                 input=historical_data_cfg,\n",
    "                                                 output=month_values_cfg)\n",
    "\n",
    "task_count_values_cfg = Config.configure_task(id=\"count_values\",\n",
    "                                                 function=count_values,\n",
    "                                                 input=month_values_cfg,\n",
    "                                                 output=nb_of_values_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18bbaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cfg = Config.configure_pipeline(id=\"my_pipeline\",\n",
    "                                         task_configs=[task_filter_current_cfg,\n",
    "                                                       task_count_values_cfg])\n",
    "\n",
    "scenario_cfg = Config.configure_scenario(id=\"my_scenario\",\n",
    "                                         pipeline_configs=[pipeline_cfg])\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_current_cfg,\n",
    "#                                                                  task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff15db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-22 16:20:03,424][Taipy][INFO] job JOB_filter_current_257edf8d-3ca3-46f5-aec6-c8a413c86c43 is completed.\n",
      "[2022-12-22 16:20:03,510][Taipy][INFO] job JOB_count_values_90c9b3c7-91e7-49ef-9064-69963d60f52a is completed.\n",
      "[2022-12-22 16:20:03,755][Taipy][INFO] job JOB_filter_current_4adc91ee-cd64-4ebf-819b-8643da0282fd is completed.\n",
      "[2022-12-22 16:20:03,901][Taipy][INFO] job JOB_count_values_968c8c34-2ed4-4f89-995c-a4137af82beb is completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PIPELINE_my_pipeline_b751f808-87de-4de1-866f-c1b3dd7bba19': [<taipy.core.job.job.Job at 0x219403b6080>,\n",
       "  <taipy.core.job.job.Job at 0x219403b7c40>]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.Core().run()\n",
    "\n",
    "scenario_1 = tp.create_scenario(scenario_cfg, creation_date=dt.datetime(2022,10,7), name=\"Scenario 2022/10/7\")\n",
    "scenario_1.submit()\n",
    "\n",
    "scenario_2 = tp.create_scenario(scenario_cfg, creation_date=dt.datetime(2022,10,7), name=\"Scenario 2022/10/7\")\n",
    "scenario_2.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4097e",
   "metadata": {},
   "source": [
    "## Cycles :\n",
    "\n",
    "So far, we have talked about how having different scenarios helps us to oversee our assumptions about the future. For example, in business, it is critical to weigh different options to come up with an optimal solution. However, this decision-making process isnâ€™t just a one-time task but rather a recurrent operation that happens over a time period. This is why we want to introduce Cycles.\n",
    "\n",
    "A cycle can be thought of as a place to store different and recurrent scenarios within a time frame. In Taipy Core, each Cycle will have a unique primary scenario representing the reference scenario for a time period.\n",
    "\n",
    "\n",
    "Typically, in a Machine Learning problem, many scenarios are created daily to predict the next day, for example. Among all those scenarios, there is only one primary scenario. In the step's example, scenarios are attached to a MONTHLY cycle. Using Cycles is useful because some specific Taipy's functions exist to navigate through these Cycles. Taipy can get all the scenarios created in a month by providing the Cycle. You can also get every primary scenario ever made to see their progress over time quickly.\n",
    "\n",
    "Moreover, nothing is more straightforward than creating a Cycle. The frequency parameter in a scenario configuration will produce the desired type of Cycle. In the code below, the scenario has a monthly cycle. When it is created, it will be attached to the correct period (month).\n",
    "\n",
    "As you can see, a Cycle can be made very easily once you have the desired frequency. In this snippet of code, since we have specified frequency=Frequency.MONTHLY, the corresponding scenario will be automatically attached to the correct period (month) once it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e09daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2022-12-15 10:12:40,104][Taipy][INFO] job JOB_filter_by_month_9d986a1d-63fd-4eb1-8f75-2544eb34424b is completed.\n",
    "#[2022-12-15 10:12:40,155][Taipy][INFO] job JOB_count_values_3b515b56-21b6-40f6-94dd-32e4a3da35dd is completed.\n",
    "#[2022-12-15 10:12:40,345][Taipy][INFO] job JOB_filter_by_month_d540c831-9d73-4555-8d69-a2a546a77467 is completed.\n",
    "#[2022-12-15 10:12:40,400][Taipy][INFO] job JOB_count_values_ec296987-6178-43cd-8f28-1a990893733a is completed.\n",
    "#[2022-12-15 10:12:40,590][Taipy][INFO] job JOB_filter_by_month_42d4f0db-d911-40f5-b820-02e750f77ba5 is completed.\n",
    "#[2022-12-15 10:12:40,643][Taipy][INFO] job JOB_count_values_53caf39c-0060-46c1-8d48-3dc6b5e74d02 is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080d69d",
   "metadata": {},
   "source": [
    "Also, as you can see every scenario has been submitted and executed entirely. However, the result for these tasks are all the same. Caching will help to skip certain redundant task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef299121",
   "metadata": {},
   "source": [
    "______________________ Taipy Studio ______________________\n",
    "- Create new file: 'config.toml'\n",
    "- Open Taipy Studio view\n",
    "- Go to the 'Config files' section of Taipy Studio\n",
    "- Right click on the right configuration\n",
    "- Choose 'Taipy: Show View'\n",
    "- Add your first Data Node by clicking the button on the right above corner of the windows\n",
    "- Create a name for it and change its details in the 'Details' section of Taipy Studio\n",
    "        - name: historical_data\n",
    "        - Details: default_path=xxxx/yyyy.csv, storage_type=csv\n",
    "- Do the same for the month_data and nb_of_values\n",
    "        - name: output\n",
    "        - Details: storage_type:pickle\n",
    "- Add a task and choose a function to associate with `<module>.<name>:function`\n",
    "        -name: filter_current\n",
    "        -Details: function=`__main__.filter_current:function`\n",
    "- Do the same for count_values\n",
    "- Link the Data Nodes and the tasks\n",
    "- Add a pipeline and link it to the tasks\n",
    "- Add a scenario and link to the pipeline\n",
    "- Add the frequency property and put \"WEEKLY:FREQUENCY\" (DAYLY, WEEKLY, MONTHLY, YEARLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65713157",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /s /q .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dedfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taipy.core.config import Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80858ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_month(df, month):\n",
    "    df['Date'] = pd.to_datetime(df['Date']) \n",
    "    df = df[df['Date'].dt.month == month]\n",
    "    return df\n",
    "\n",
    "def count_values(df):\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba5a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_cfg = Config.configure_csv_data_node(id=\"historical_data\",\n",
    "                                                     default_path=\"time_series.csv\")\n",
    "month_cfg =  Config.configure_data_node(id=\"month\")\n",
    "month_values_cfg =  Config.configure_data_node(id=\"month_data\")\n",
    "nb_of_values_cfg = Config.configure_data_node(id=\"nb_of_values\")\n",
    "\n",
    "\n",
    "task_filter_by_month_cfg = Config.configure_task(id=\"filter_by_month\",\n",
    "                                                 function=filter_by_month,\n",
    "                                                 input=[historical_data_cfg, month_cfg],\n",
    "                                                 output=month_values_cfg)\n",
    "\n",
    "task_count_values_cfg = Config.configure_task(id=\"count_values\",\n",
    "                                                 function=count_values,\n",
    "                                                 input=month_values_cfg,\n",
    "                                                 output=nb_of_values_cfg)\n",
    "\n",
    "pipeline_cfg = Config.configure_pipeline(id=\"my_pipeline\",\n",
    "                                         task_configs=[task_filter_by_month_cfg,\n",
    "                                                       task_count_values_cfg])\n",
    "\n",
    "scenario_cfg = Config.configure_scenario(id=\"my_scenario\",\n",
    "                                         pipeline_configs=[pipeline_cfg],\n",
    "                                         frequency=Frequency.MONTHLY)\n",
    "\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_by_month_cfg,\n",
    "#                                                    task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b28decd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.Core().run()\n",
    "\n",
    "scenario_1 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,10,7),\n",
    "                                name=\"Scenario 2022/10/7\")\n",
    "scenario_2 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,10,5),\n",
    "                                name=\"Scenario 2022/10/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be67d3",
   "metadata": {},
   "source": [
    "Scenario 1 and 2 belongs to the same cycle but they don't share the same data node. Each one have a Data Node by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c286c81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month Data Node of Scenario 1 10\n",
      "Month Data Node of Scenario 2 10\n",
      "[2022-12-22 16:20:04,746][Taipy][INFO] job JOB_filter_by_month_a4d3c4a7-5ec9-4cca-8a1b-578c910e255a is completed.\n",
      "[2022-12-22 16:20:04,833][Taipy][INFO] job JOB_count_values_a81b2f60-e9f9-4848-aa58-272810a0b755 is completed.\n",
      "[2022-12-22 16:20:05,026][Taipy][INFO] job JOB_filter_by_month_22a3298b-ac8d-4b55-b51f-5fab0971cc9e is completed.\n",
      "[2022-12-22 16:20:05,084][Taipy][INFO] job JOB_count_values_a52b910a-4024-443e-8ea2-f3cdda6c1c9d is completed.\n",
      "[2022-12-22 16:20:05,317][Taipy][INFO] job JOB_filter_by_month_8643e5cf-e863-434f-a1ba-18222d6faab8 is completed.\n",
      "[2022-12-22 16:20:05,376][Taipy][INFO] job JOB_count_values_72ab71be-f923-4898-a8a8-95ec351c24d9 is completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PIPELINE_my_pipeline_8f1e1475-9294-41be-a9da-70539491524a': [<taipy.core.job.job.Job at 0x21940433580>,\n",
       "  <taipy.core.job.job.Job at 0x21940431750>]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_1.month.write(10)\n",
    "scenario_2.month.write(10)\n",
    "\n",
    "\n",
    "print(\"Month Data Node of Scenario 1\", scenario_1.month.read())\n",
    "print(\"Month Data Node of Scenario 2\", scenario_2.month.read())\n",
    "\n",
    "scenario_1.submit()\n",
    "scenario_2.submit()\n",
    "\n",
    "scenario_3 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2021,9,1),\n",
    "                                name=\"Scenario 2022/9/1\")\n",
    "scenario_3.month.write(9)\n",
    "scenario_3.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96962e",
   "metadata": {},
   "source": [
    "## Scoping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04fbc3",
   "metadata": {},
   "source": [
    "Scoping determines how Data Nodes are shared between cycles, scenarios, and pipelines. Indeed, multiple scenarios can have their own Data Nodes or share the same one. For example, the initial/historical dataset is usually shared by all the scenarios/pipelines/cycles. It has a Global Scope and will be unique in the entire application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e7161d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /s /q .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c22fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taipy.core.config import Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e0655a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_month(df, month):\n",
    "    df['Date'] = pd.to_datetime(df['Date']) \n",
    "    df = df[df['Date'].dt.month == month]\n",
    "    return df\n",
    "\n",
    "def count_values(df):\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeab6f9",
   "metadata": {},
   "source": [
    "- **Pipeline** scope: two pipelines can reference different Data Nodes even if their names are the same. For example, the _prediction_ Data Node of an ARIMA model (ARIMA pipeline) and the _prediction_ Data Node of a RandomForest model (RandomForest pipeline). \n",
    "\n",
    "- **Scenario** scope: pipelines share the same Data Node within a scenario. \n",
    "\n",
    "- **Cycle** scope: scenarios from the same cycle share the same Data Node\n",
    "\n",
    "- **Global** scope: unique Data Node for all the scenarios/pipelines/cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac180e",
   "metadata": {},
   "source": [
    "______________________ Taipy Studio ______________________\n",
    "- Create new file: 'config.toml'\n",
    "- Open Taipy Studio view\n",
    "- Go to the 'Config files' section of Taipy Studio\n",
    "- Right click on the right configuration\n",
    "- Choose 'Taipy: Show View'\n",
    "- Add your first Data Node by clicking the button on the right above corner of the windows\n",
    "- Create a name for it and change its details in the 'Details' section of Taipy Studio\n",
    "        - name: historical_data\n",
    "        - Details: default_path=xxxx/yyyy.csv, storage_type=csv\n",
    "- Do the same for the month_data and nb_of_values\n",
    "        - name: output\n",
    "        - Details: storage_type:pickle\n",
    "- Add a task and choose a function to associate with `<module>.<name>:function`\n",
    "        -name: filter_current\n",
    "        -Details: function=`__main__.filter_current:function`\n",
    "- Do the same for count_values\n",
    "- Link the Data Nodes and the tasks\n",
    "- Add a pipeline and link it to the tasks\n",
    "- Add a scenario and link to the pipeline\n",
    "- Add the frequency property and put \"WEEKLY:FREQUENCY\" (DAILY, WEEKLY, MONTHLY, YEARLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36fb1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_cfg = Config.configure_csv_data_node(id=\"historical_data\",\n",
    "                                                 default_path=\"time_series.csv\",\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "month_cfg =  Config.configure_data_node(id=\"month\", scope=Scope.CYCLE)\n",
    "\n",
    "month_values_cfg = Config.configure_data_node(id=\"month_data\",\n",
    "                                               scope=Scope.CYCLE)\n",
    "nb_of_values_cfg = Config.configure_data_node(id=\"nb_of_values\")\n",
    "\n",
    "\n",
    "task_filter_by_month_cfg = Config.configure_task(id=\"filter_by_month\",\n",
    "                                                 function=filter_by_month,\n",
    "                                                 input=[historical_data_cfg,month_cfg],\n",
    "                                                 output=month_values_cfg)\n",
    "\n",
    "task_count_values_cfg = Config.configure_task(id=\"count_values\",\n",
    "                                                 function=count_values,\n",
    "                                                 input=month_values_cfg,\n",
    "                                                 output=nb_of_values_cfg)\n",
    "\n",
    "pipeline_cfg = Config.configure_pipeline(id=\"my_pipeline\",\n",
    "                                         task_configs=[task_filter_by_month_cfg,\n",
    "                                                       task_count_values_cfg])\n",
    "\n",
    "scenario_cfg = Config.configure_scenario(id=\"my_scenario\",\n",
    "                                         pipeline_configs=[pipeline_cfg],\n",
    "                                         frequency=Frequency.MONTHLY)\n",
    "\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_by_month_cfg,\n",
    "#                                                                  task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d54956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.Core().run()\n",
    "\n",
    "scenario_1 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,10,7),\n",
    "                                name=\"Scenario 2022/10/7\")\n",
    "scenario_2 = tp.create_scenario(scenario_cfg,\n",
    "                               creation_date=dt.datetime(2022,10,5),\n",
    "                               name=\"Scenario 2022/10/5\")\n",
    "scenario_3 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2021,9,1),\n",
    "                                name=\"Scenario 2021/9/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff689399",
   "metadata": {},
   "source": [
    "Scenario 1 and 2 belongs to the same cycle so I can define the month just once for scenario 1 and 2 because month has a Cycle scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9782b0",
   "metadata": {},
   "source": [
    "![]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df4656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: month 10\n",
      "Scenario 2: month 10\n"
     ]
    }
   ],
   "source": [
    "scenario_1.month.write(10)\n",
    "print(\"Scenario 1: month\", scenario_1.month.read())\n",
    "print(\"Scenario 2: month\", scenario_2.month.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1ca569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 1: submit\n",
      "[2022-12-22 16:20:05,810][Taipy][INFO] job JOB_filter_by_month_d71cfd10-f674-40c8-b7a5-c66bea8773ef is completed.\n",
      "[2022-12-22 16:20:05,902][Taipy][INFO] job JOB_count_values_cbe0b3b3-2531-440a-9413-48845c9cfdf1 is completed.\n",
      "Value 849\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScenario 1: submit\")\n",
    "scenario_1.submit()\n",
    "print(\"Value\", scenario_1.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07520802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 2: first submit\n",
      "[2022-12-22 16:20:06,101][Taipy][INFO] job JOB_filter_by_month_2e474e1b-dc0b-464c-8d14-d64a59535717 is completed.\n",
      "[2022-12-22 16:20:06,162][Taipy][INFO] job JOB_count_values_259039e2-20ed-4400-b11a-06119939f081 is completed.\n",
      "Value 849\n",
      "Scenario 2: second submit\n",
      "[2022-12-22 16:20:06,356][Taipy][INFO] job JOB_filter_by_month_705fcb69-64fc-4f66-a5f3-90169e09f8bf is completed.\n",
      "[2022-12-22 16:20:06,426][Taipy][INFO] job JOB_count_values_5a8eea88-4477-48ab-a401-8036bada2267 is completed.\n",
      "Value 849\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScenario 2: first submit\")\n",
    "scenario_2.submit()\n",
    "print(\"Value\", scenario_2.nb_of_values.read())\n",
    "print(\"Scenario 2: second submit\")\n",
    "scenario_2.submit()\n",
    "print(\"Value\", scenario_2.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7db125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 3: submit\n",
      "[2022-12-22 16:20:07,404][Taipy][INFO] job JOB_filter_by_month_d0eefd6f-af96-46b8-b916-7cb94303ae4d is completed.\n",
      "[2022-12-22 16:20:07,578][Taipy][INFO] job JOB_count_values_469f6ae3-7a8a-4b65-9175-899399013f60 is completed.\n",
      "Value 1012\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScenario 3: submit\")\n",
    "scenario_3.month.write(9)\n",
    "scenario_3.submit()\n",
    "print(\"Value\", scenario_3.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3564e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 3: change in historical data\n",
      "[2022-12-22 16:20:08,150][Taipy][INFO] job JOB_filter_by_month_8bab73fb-733b-4d8e-93fb-c550213bb2f1 is completed.\n",
      "[2022-12-22 16:20:08,300][Taipy][INFO] job JOB_count_values_909c5bfa-a622-4959-aa2e-864abef8a0b8 is completed.\n",
      "Value 1012\n"
     ]
    }
   ],
   "source": [
    "print(\"Scenario 3: change in historical data\")\n",
    "scenario_3.historical_data.write(pd.read_csv('time_series_2.csv'))\n",
    "scenario_3.submit()\n",
    "print(\"Value\", scenario_3.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db78c1d",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832d9ca",
   "metadata": {},
   "source": [
    "Caching is an important feature of Taipy. Tasks can be skipped depending if input data nodes of tasks have changed or not. If none of the input data nodes have been changed after a first submission, tasks will be skipped saving time and ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "877e0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /s /q .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "477242d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_month(df, month):\n",
    "    df['Date'] = pd.to_datetime(df['Date']) \n",
    "    df = df[df['Date'].dt.month == month]\n",
    "    return df\n",
    "\n",
    "def count_values(df):\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc99a0",
   "metadata": {},
   "source": [
    "______________________ Taipy Studio ______________________\n",
    "- Create new file: 'config.toml'\n",
    "- Open Taipy Studio view\n",
    "- Go to the 'Config files' section of Taipy Studio\n",
    "- Right click on the right configuration\n",
    "- Choose 'Taipy: Show View'\n",
    "- Add your first Data Node by clicking the button on the right above corner of the windows\n",
    "- Create a name for it and change its details in the 'Details' section of Taipy Studio\n",
    "        - name: historical_data\n",
    "        - Details: default_path=xxxx/yyyy.csv, storage_type=csv\n",
    "- Do the same for the month_data and nb_of_values\n",
    "        - name: output\n",
    "        - Details: storage_type:pickle, cacheable=True\n",
    "- Add a task and choose a function to associate with `<module>.<name>:function`\n",
    "        -name: filter_current\n",
    "        -Details: function=`__main__.filter_current:function`\n",
    "- Do the same for count_values\n",
    "- Link the Data Nodes and the tasks\n",
    "- Add a pipeline and link it to the tasks\n",
    "- Add a scenario and link to the pipeline\n",
    "- Add the frequency property and put \"WEEKLY:FREQUENCY\" (DAILY, WEEKLY, MONTHLY, YEARLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f8907",
   "metadata": {},
   "source": [
    "The configuration is the same. 'cacheabable' are added to the output Data Nodes that we want to be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90a52f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_cfg = Config.configure_csv_data_node(id=\"historical_data\",\n",
    "                                                 default_path=\"time_series.csv\",\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "month_cfg =  Config.configure_data_node(id=\"month\", scope=Scope.CYCLE)\n",
    "month_values_cfg =  Config.configure_data_node(id=\"month_data\",\n",
    "                                               scope=Scope.CYCLE,\n",
    "                                               cacheable=True)\n",
    "\n",
    "nb_of_values_cfg = Config.configure_data_node(id=\"nb_of_values\",\n",
    "                                              cacheable=True)\n",
    "\n",
    "\n",
    "task_filter_by_month_cfg = Config.configure_task(id=\"filter_by_month\",\n",
    "                                                 function=filter_by_month,\n",
    "                                                 input=[historical_data_cfg, month_cfg],\n",
    "                                                 output=month_values_cfg)\n",
    "\n",
    "task_count_values_cfg = Config.configure_task(id=\"count_values\",\n",
    "                                                 function=count_values,\n",
    "                                                 input=month_values_cfg,\n",
    "                                                 output=nb_of_values_cfg)\n",
    "\n",
    "pipeline_cfg = Config.configure_pipeline(id=\"my_pipeline\",\n",
    "                                         task_configs=[task_filter_by_month_cfg,\n",
    "                                                       task_count_values_cfg])\n",
    "\n",
    "scenario_cfg = Config.configure_scenario(id=\"my_scenario\",\n",
    "                                         pipeline_configs=[pipeline_cfg],\n",
    "                                         frequency=Frequency.MONTHLY)\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_by_month_cfg,\n",
    "#                                                    task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d6210",
   "metadata": {},
   "source": [
    "Creation of three different scenarios with different creation dates and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b31563",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.Core().run()\n",
    "\n",
    "scenario_1 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,10,7),\n",
    "                                name=\"Scenario 2022/10/7\")\n",
    "scenario_2 = tp.create_scenario(scenario_cfg,\n",
    "                               creation_date=dt.datetime(2022,10,5),\n",
    "                               name=\"Scenario 2022/10/5\")\n",
    "scenario_3 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2021,9,1),\n",
    "                                name=\"Scenario 2022/9/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a44abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: month 10\n",
      "Scenario 2: month 10\n"
     ]
    }
   ],
   "source": [
    "# scenario 1 and 2 belongs to the same cycle so \n",
    "# defining the month for scenario 1 defines the month for the scenarios in the cycle\n",
    "scenario_1.month.write(10)\n",
    "print(\"Scenario 1: month\", scenario_1.month.read())\n",
    "print(\"Scenario 2: month\", scenario_2.month.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e0a60",
   "metadata": {},
   "source": [
    " No task has already been submitted so everything will be submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e159015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: submit\n",
      "[2022-12-22 16:20:09,079][Taipy][INFO] job JOB_filter_by_month_0d7836eb-70eb-4fe6-b954-0e56967831b6 is completed.\n",
      "[2022-12-22 16:20:09,177][Taipy][INFO] job JOB_count_values_91214241-ce81-42d8-9025-e83509652133 is completed.\n",
      "Value 849\n"
     ]
    }
   ],
   "source": [
    "print(\"Scenario 1: submit\")\n",
    "scenario_1.submit()\n",
    "print(\"Value\", scenario_1.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b454d",
   "metadata": {},
   "source": [
    "When the second scenario is being executed, the first task will be skipped. Indeed, the two scenarios shares the same data nodes for this task and no input data nodes have been changed. node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "033d68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 2: first submit\n",
      "[2022-12-22 16:20:09,317][Taipy][INFO] job JOB_filter_by_month_c1db1f0c-6e0a-4691-b0a3-331d473c4c42 is skipped.\n",
      "[2022-12-22 16:20:09,371][Taipy][INFO] job JOB_count_values_271cefd0-8648-47fa-8948-ed49e93e3eee is completed.\n",
      "Value 849\n"
     ]
    }
   ],
   "source": [
    "# first task has already been executed by scenario 1 because scenario 2 shares the same data node for this task\n",
    "print(\"Scenario 2: first submit\")\n",
    "scenario_2.submit()\n",
    "print(\"Value\", scenario_2.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405d699",
   "metadata": {},
   "source": [
    "Resubmitting the same scenario without any change will just skip every task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6da8443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 2: second submit\n",
      "[2022-12-22 16:20:09,516][Taipy][INFO] job JOB_filter_by_month_da2762d1-6f24-40c1-9bd1-d6786fee7a8d is skipped.\n",
      "[2022-12-22 16:20:09,546][Taipy][INFO] job JOB_count_values_9071dff4-37b2-4095-a7ed-34ef81daad27 is skipped.\n",
      "Value 849\n"
     ]
    }
   ],
   "source": [
    "# every task has already been executed so everything will be skipped\n",
    "print(\"Scenario 2: second submit\")\n",
    "scenario_2.submit()\n",
    "print(\"Value\", scenario_2.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec6ece",
   "metadata": {},
   "source": [
    "This scenario is not in the same cycle. We change the month to 9 and every task will be completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2798af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 3: submit\n",
      "[2022-12-22 16:20:10,071][Taipy][INFO] job JOB_filter_by_month_c4d06eba-a149-4b79-9194-78972c7b7a18 is completed.\n",
      "[2022-12-22 16:20:10,257][Taipy][INFO] job JOB_count_values_817df173-6bae-4742-a2c0-b8b8eba52872 is completed.\n",
      "Value 1012\n"
     ]
    }
   ],
   "source": [
    "# scenario 3 has no connection to the other scenarios so everything will be executed\n",
    "print(\"Scenario 3: submit\")\n",
    "scenario_3.month.write(9)\n",
    "scenario_3.submit()\n",
    "print(\"Value\", scenario_3.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9f288",
   "metadata": {},
   "source": [
    "Here, we change the input data node of the pipeline so Taipy will re run the correct tasks to make sure that everything is up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "464b8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 3: change in historical data\n",
      "[2022-12-22 16:20:10,870][Taipy][INFO] job JOB_filter_by_month_92f32135-b410-41f0-b9f3-a852c2eb07cd is completed.\n",
      "[2022-12-22 16:20:10,932][Taipy][INFO] job JOB_count_values_a6a75e13-4cd4-4f7e-bc4e-d14a86733440 is completed.\n",
      "Value 1012\n"
     ]
    }
   ],
   "source": [
    "# changing an input data node will make the task be reexecuted\n",
    "print(\"Scenario 3: change in historical data\")\n",
    "scenario_3.historical_data.write(pd.read_csv('time_series_2.csv'))\n",
    "scenario_3.submit()\n",
    "print(\"Value\", scenario_3.nb_of_values.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d34e32",
   "metadata": {},
   "source": [
    "## Execution modes\n",
    "\n",
    "Taipy has different ways to execute the code. There is two different job execution modes:\n",
    "- standalone: asynchronous. Jobs can be runned in parallel depending on the graph of execution if max_nb_of_workers > 1\n",
    "- development mode: synchronous\n",
    "\n",
    "Options of submit:\n",
    "- wait: if wait is True, the submit is synchronous and will wait for the end of all the jobs (if timeout is not defined)\n",
    "- timeout: if wait is True, Taipy will wait for the end of the submit until a certain amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47223948",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /s /q .data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a120fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Taipy Core Data nodes - CSV, pickle\n",
    "from taipy.core.config import Config, Scope, Frequency\n",
    "import taipy as tp\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08105ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_month(df, month):\n",
    "    df['Date'] = pd.to_datetime(df['Date']) \n",
    "    df = df[df['Date'].dt.month == month]\n",
    "    return df\n",
    "\n",
    "def count_values(df):\n",
    "    print(\"Wait 10 seconds\")\n",
    "    time.sleep(10)\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f20e400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<taipy.core.config.job_config.JobConfig at 0x2193e63c3d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.configure_job_executions(mode=\"standalone\", max_nb_of_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a99b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_cfg = Config.configure_csv_data_node(id=\"historical_data\",\n",
    "                                                 default_path=\"time_series.csv\",\n",
    "                                                 scope=Scope.GLOBAL)\n",
    "\n",
    "month_cfg = Config.configure_data_node(id=\"month\",\n",
    "                                       scope=Scope.CYCLE)\n",
    "\n",
    "month_values_cfg =  Config.configure_data_node(id=\"month_data\",\n",
    "                                               scope=Scope.CYCLE,\n",
    "                                               cacheable=True)\n",
    "\n",
    "nb_of_values_cfg = Config.configure_data_node(id=\"nb_of_values\",\n",
    "                                              cacheable=True)\n",
    "\n",
    "\n",
    "task_filter_by_month_cfg = Config.configure_task(id=\"filter_by_month\",\n",
    "                                                 function=filter_by_month,\n",
    "                                                 input=[historical_data_cfg, month_cfg],\n",
    "                                                 output=month_values_cfg)\n",
    "\n",
    "task_count_values_cfg = Config.configure_task(id=\"count_values\",\n",
    "                                                 function=count_values,\n",
    "                                                 input=month_values_cfg,\n",
    "                                                 output=nb_of_values_cfg)\n",
    "\n",
    "pipeline_cfg = Config.configure_pipeline(id=\"my_pipeline\",\n",
    "                                         task_configs=[task_filter_by_month_cfg,\n",
    "                                                       task_count_values_cfg])\n",
    "\n",
    "scenario_cfg = Config.configure_scenario(id=\"my_scenario\",\n",
    "                                         pipeline_configs=[pipeline_cfg],\n",
    "                                         frequency=Frequency.MONTHLY)\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_by_month_cfg,\n",
    "#                                                    task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "538da846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x21940406590 state=finished raised PicklingError>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 330, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\_scheduler\\_dispatcher\\_standalone_job_dispatcher.py\", line 53, in _update_job_status_from_future\n",
      "    self._update_job_status(job, ft.result())\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n",
      "exception calling callback for <Future at 0x219406d23e0 state=finished raised PicklingError>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 330, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\_scheduler\\_dispatcher\\_standalone_job_dispatcher.py\", line 53, in _update_job_status_from_future\n",
      "    self._update_job_status(job, ft.result())\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    tp.Core().run()\n",
    "    scenario_1 = tp.create_scenario(scenario_cfg, creation_date=dt.datetime(2022,10,7), name=\"Scenario 2022/10/7\")\n",
    "    scenario_1.month.write(10)\n",
    "    scenario_1.submit()\n",
    "    scenario_1.submit()\n",
    "\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "253faa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x21940367790 state=finished raised PicklingError>\n",
      "concurrent.futures.process._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 330, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\_scheduler\\_dispatcher\\_standalone_job_dispatcher.py\", line 53, in _update_job_status_from_future\n",
      "    self._update_job_status(job, ft.result())\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\queues.py\", line 245, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"C:\\Users\\jacta\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\multiprocessing\\reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "_pickle.PicklingError: Can't pickle <function filter_by_month at 0x00000219403DDA20>: it's not the same object as __main__.filter_by_month\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m scenario_1 \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mcreate_scenario(scenario_cfg, creation_date\u001b[38;5;241m=\u001b[39mdt\u001b[38;5;241m.\u001b[39mdatetime(\u001b[38;5;241m2022\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m7\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario 2022/10/7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m scenario_1\u001b[38;5;241m.\u001b[39mmonth\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m scenario_1\u001b[38;5;241m.\u001b[39msubmit(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m scenario_1\u001b[38;5;241m.\u001b[39msubmit(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\scenario\\scenario.py:277\u001b[0m, in \u001b[0;36mScenario.submit\u001b[1;34m(self, force, wait, timeout)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"Submit this scenario for execution.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03mAll the `Task^`s of the scenario will be submitted for execution.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m    timeout (Union[float, int]): The optional maximum number of seconds to wait for the jobs to be finished before returning.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tp\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\taipy.py:76\u001b[0m, in \u001b[0;36msubmit\u001b[1;34m(entity, force, wait, timeout)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"Submit an entity for execution.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mIf the entity is a pipeline or a scenario, all the tasks of the entity are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entity, Scenario):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ScenarioManagerFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entity, Pipeline):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _PipelineManagerFactory\u001b[38;5;241m.\u001b[39m_build_manager()\u001b[38;5;241m.\u001b[39m_submit(entity, force\u001b[38;5;241m=\u001b[39mforce, wait\u001b[38;5;241m=\u001b[39mwait, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\scenario\\_scenario_manager.py:135\u001b[0m, in \u001b[0;36m_ScenarioManager._submit\u001b[1;34m(cls, scenario, force, wait, timeout)\u001b[0m\n\u001b[0;32m    133\u001b[0m jobs_in_pipelines \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pipeline \u001b[38;5;129;01min\u001b[39;00m scenario\u001b[38;5;241m.\u001b[39mpipelines\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 135\u001b[0m     jobs_in_pipelines[pipeline\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m \u001b[43m_PipelineManagerFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jobs_in_pipelines\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\pipeline\\_pipeline_manager.py:107\u001b[0m, in \u001b[0;36m_PipelineManager._submit\u001b[1;34m(cls, pipeline, callbacks, force, wait, timeout)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NonExistingPipeline(pipeline_id)\n\u001b[0;32m    105\u001b[0m pipeline_subscription_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__get_status_notifier_callbacks(pipeline) \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 107\u001b[0m     \u001b[43m_TaskManagerFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_subscription_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\_scheduler\\_scheduler.py:82\u001b[0m, in \u001b[0;36m_Scheduler.submit\u001b[1;34m(cls, pipeline, callbacks, force, wait, timeout)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wait_until_job_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\notebook\\lib\\site-packages\\taipy\\core\\_scheduler\\_scheduler.py:165\u001b[0m, in \u001b[0;36m_Scheduler.__wait_until_job_finished\u001b[1;34m(cls, jobs, timeout)\u001b[0m\n\u001b[0;32m    163\u001b[0m         index \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m         \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Limit CPU usage\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    tp.Core().run()\n",
    "    scenario_1 = tp.create_scenario(scenario_cfg, creation_date=dt.datetime(2022,10,7), name=\"Scenario 2022/10/7\")\n",
    "    scenario_1.month.write(10)\n",
    "    scenario_1.submit(wait=True)\n",
    "    scenario_1.submit(wait=True, timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e351d",
   "metadata": {},
   "source": [
    "## Callback on scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a5f71",
   "metadata": {},
   "source": [
    "To have an action after the change of status of a job, we can subscribe a function to a scenario. This function will be called each time a job has its status changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f90b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_scenario_state(scenario, job):\n",
    "    \"\"\"All the scenarios are subscribed to the callback_scenario_state function. It means whenever a job is done, it is called.\n",
    "    Depending on the job and the status, it will update the message stored in a json that is then displayed on the GUI.\n",
    "\n",
    "    Args:\n",
    "        scenario (Scenario): the scenario of the job changed\n",
    "        job (_type_): the job that has its status changed\n",
    "    \"\"\"\n",
    "    print(scenario.name)\n",
    "    if job.status.value == 7:\n",
    "        for data_node in job.task.output.values():\n",
    "            print(data_node.read())\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    tp.Core().run()\n",
    "    scenario_1 = tp.create_scenario(scenario_cfg, creation_date=dt.datetime(2022,10,7), name=\"Scenario 2022/10/7\")\n",
    "    scenario_1.subscribe(callback_scenario_state)\n",
    "\n",
    "    scenario_1.submit(wait=True)\n",
    "    scenario_1.submit(wait=True, timeout=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cf109",
   "metadata": {},
   "source": [
    "## Comparison of funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce00b74",
   "metadata": {},
   "source": [
    "Taipy provides a way to compare scenarios by providing a function directly into the configuration of the scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c6222",
   "metadata": {},
   "source": [
    "_data_node_results_ is a list of data nodes from all scenarios passed in the comparator. We iterate through it to compare scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_function(*data_node_results):\n",
    "    compare_result= {}\n",
    "    current_res_i = 0\n",
    "    for current_res in data_node_results:\n",
    "        compare_result[current_res_i]={}\n",
    "        next_res_i = 0\n",
    "        for next_res in data_node_results:\n",
    "            print(f\"comparing result {current_res_i} with result {next_res_i}\")\n",
    "            compare_result[current_res_i][next_res_i] = next_res - current_res\n",
    "            next_res_i += 1\n",
    "        current_res_i += 1\n",
    "    return compare_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fe5dd",
   "metadata": {},
   "source": [
    "The Data Node that will be compared here is the 'month' Data Node. It is indicated in the comparators parameter of the _configure_scenario_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_cfg = Config.configure_scenario(\"multiply_scenario\",\n",
    "                                         [pipeline_cfg],\n",
    "                                         comparators={month_cfg.id: compare_function},\n",
    "                                         frequency=Frequency.MONTHLY)\n",
    "\n",
    "#scenario_cfg = Config.configure_scenario_from_tasks(id=\"my_scenario\",\n",
    "#                                                    task_configs=[task_filter_by_month_cfg,\n",
    "#                                                                  task_count_values_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a27c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.Core().run()\n",
    "\n",
    "scenario_1 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,10,7),\n",
    "                                name=\"Scenario 2022/10/7\")\n",
    "scenario_2 = tp.create_scenario(scenario_cfg,\n",
    "                                creation_date=dt.datetime(2022,8,5),\n",
    "                                name=\"Scenario 2022/8/5\")\n",
    "\n",
    "scenario_1.month.write(10)\n",
    "scenario_2.month.write(8)\n",
    "print(\"Scenario 1: month\", scenario_1.month.read())\n",
    "print(\"Scenario 2: month\", scenario_2.month.read())\n",
    "\n",
    "print(\"\\nScenario 1: submit\")\n",
    "scenario_1.submit()\n",
    "print(\"Value\", scenario_1.nb_of_values.read())\n",
    "\n",
    "print(\"\\nScenario 2: first submit\")\n",
    "scenario_2.submit()\n",
    "print(\"Value\", scenario_2.nb_of_values.read())\n",
    "\n",
    "\n",
    "print(tp.compare_scenarios(scenario_1, scenario_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f170174",
   "metadata": {},
   "source": [
    "## Taipy Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c540edf",
   "metadata": {},
   "source": [
    "Taipy Rest allows the user to navigate through the entities of the application but also create and submit scenarios. Try the following commands:\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e42d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.Rest().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e306d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0ce91b604fe24c892c929035f6db55d5731bc763742badb24bf85fa5ee76a5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
